tags:: [[Computer Science - Computation and Language]], [[Computer Science - Machine Learning]]
date:: [[Oct 27th, 2021]]
extra:: arXiv: 2107.02173
title:: @Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence
item-type:: [[journalArticle]]
access-date:: 2021-11-10T09:30:13Z
original-title:: Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence
url:: http://arxiv.org/abs/2107.02173
short-title:: Is Automated Topic Model Evaluation Broken?
publication-title:: arXiv:2107.02173 [cs]
authors:: [[Alexander Hoyle]], [[Pranav Goel]], [[Denis Peskov]], [[Andrew Hian-Cheong]], [[Jordan Boyd-Graber]], [[Philip Resnik]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/groups/2386895/items/2EN5SDYS), [Web library](https://www.zotero.org/groups/2386895/items/2EN5SDYS)

- [[Abstract]]
	- Topic model evaluation, like evaluation of other unsupervised methods, can be contentious. However, the field has coalesced around automated estimates of topic coherence, which rely on the frequency of word co-occurrences in a reference corpus. Contemporary neural topic models surpass classical ones according to these metrics. At the same time, topic model evaluation suffers from a validation gap: automated coherence, developed for classical models, has not been validated using human experimentation for neural models. In addition, a meta-analysis of topic modeling literature reveals a substantial standardization gap in automated topic modeling benchmarks. To address the validation gap, we compare automated coherence with the two most widely accepted human judgment tasks: topic rating and word intrusion. To address the standardization gap, we systematically evaluate a dominant classical model and two state-of-the-art neural models on two commonly used datasets. Automated evaluations declare a winning model when corresponding human evaluations do not, calling into question the validity of fully automatic evaluations independent of human judgments.
- [[Attachments]]
	- [arXiv.org Snapshot](https://arxiv.org/abs/2107.02173) {{zotero-imported-file MBYTBZER, "2107.html"}}
	- [arXiv Fulltext PDF](https://arxiv.org/pdf/2107.02173.pdf) {{zotero-imported-file HEP2BSZH, "Hoyle et al. - 2021 - Is Automated Topic Model Evaluation Broken The I.pdf"}}
- [[Notes]]
	- Comment: Accepted to NeurIPS 2021 (spotlight presentation). CR version