date:: [[Sep 5th, 2021]]
publisher:: ACM
place:: Lausanne Switzerland
conference-name:: HIP '21: The 6th International Workshop on Historical Document Imaging and Processing
proceedings-title:: The 6th International Workshop on Historical Document Imaging and Processing
isbn:: 978-1-4503-8690-6
doi:: 10.1145/3476887.3476888
title:: @A survey of OCR evaluation tools and metrics
pages:: 13-18
item-type:: [[conferencePaper]]
access-date:: 2023-06-21T06:21:55Z
original-title:: A survey of OCR evaluation tools and metrics
language:: en
url:: https://dl.acm.org/doi/10.1145/3476887.3476888
authors:: [[Clemens Neudecker]], [[Konstantin Baierer]], [[Mike Gerber]], [[Christian Clausner]], [[Apostolos Antonacopoulos]], [[Stefan Pletschacher]]
library-catalog:: DOI.org (Crossref)
links:: [Local library](zotero://select/groups/2386895/items/SCKVUZ2N), [Web library](https://www.zotero.org/groups/2386895/items/SCKVUZ2N)

- [[Abstract]]
	- The millions of pages of historical documents that are digitized in libraries are increasingly used in contexts that have more specific requirements for OCR quality than keyword search. How to comprehensively, efficiently and reliably assess the quality of OCR results against the background of mass digitization, when ground truth can only ever be produced for very small numbers? Due to gaps in specifications, results from OCR evaluation tools can return different results, and due to differences in implementation, even commonly used error rates are often not directly comparable. OCR evaluation metrics and sampling methods are also not sufficient where they do not take into account the accuracy of layout analysis, since for advanced use cases like Natural Language Processing or the Digital Humanities, accurate layout analysis and detection of the reading order are crucial. We provide an overview of OCR evaluation metrics and tools, describe two advanced use cases for OCR results, and perform an OCR evaluation experiment with multiple evaluation tools and different metrics for two distinct datasets. We analyze the differences and commonalities in light of the presented use cases and suggest areas for future work.
- [[Attachments]]
	- [Neudecker et al. - 2021 - A survey of OCR evaluation tools and metrics.pdf](https://primaresearch.org/www/assets/papers/HIP21_CNeudecker_OcrEvalSurvey.pdf) {{zotero-imported-file U4MYWZ6K, "Neudecker et al. - 2021 - A survey of OCR evaluation tools and metrics.pdf"}}
- tags:: [[accuracy]], [[evaluation]], [[metrics]], [[optical character recognition]]
  date:: [[Sep 5th, 2021]]
  publisher:: Association for Computing Machinery
  place:: "New York, NY, USA"
  isbn:: 978-1-4503-8690-6
  title:: @A survey of OCR evaluation tools and metrics
  book-title:: The 6th International Workshop on Historical Document Imaging and Processing
  pages:: 13â€“18
  item-type:: [[bookSection]]
  access-date:: 2022-02-03
  original-title:: A survey of OCR evaluation tools and metrics
  url:: https://doi.org/10.1145/3476887.3476888
  authors:: [[Clemens Neudecker]], [[Konstantin Baierer]], [[Mike Gerber]], [[Clausner Christian]], [[Antonacopoulos Apostolos]], [[Pletschacher Stefan]]
  library-catalog:: ACM Digital Library
  links:: [Local library](zotero://select/groups/2386895/items/3I4DVSJY), [Web library](https://www.zotero.org/groups/2386895/items/3I4DVSJY)
- [[Abstract]]
	- The millions of pages of historical documents that are digitized in libraries are increasingly used in contexts that have more specific requirements for OCR quality than keyword search. How to comprehensively, efficiently and reliably assess the quality of OCR results against the background of mass digitization, when ground truth can only ever be produced for very small numbers? Due to gaps in specifications, results from OCR evaluation tools can return different results, and due to differences in implementation, even commonly used error rates are often not directly comparable. OCR evaluation metrics and sampling methods are also not sufficient where they do not take into account the accuracy of layout analysis, since for advanced use cases like Natural Language Processing or the Digital Humanities, accurate layout analysis and detection of the reading order are crucial. We provide an overview of OCR evaluation metrics and tools, describe two advanced use cases for OCR results, and perform an OCR evaluation experiment with multiple evaluation tools and different metrics for two distinct datasets. We analyze the differences and commonalities in light of the presented use cases and suggest areas for future work.