date:: 2020
publisher:: CEUR-WS
conference-name:: Proceedings of the 5th Swiss Text Analytics Conference (SwissText) & 16th Conference on Natural Language Processing (KONVENS)
extra:: ISSN: 1613-0073
title:: @To BERT or not to BERT – Comparing contextual embeddings in a deep learning architecture for the automatic recognition of four types of speech, thought and writing representation
item-type:: [[conferencePaper]]
access-date:: 2023-07-17T08:13:47Z
rights:: https://creativecommons.org/licenses/by/4.0/
original-title:: "To BERT or not to BERT – Comparing contextual embeddings in a deep learning architecture for the automatic recognition of four types of speech, thought and writing representation"
language:: eng
url:: https://ids-pub.bsz-bw.de/frontdoor/index/index/docId/11561
authors:: [[Annelen Brunner]], [[Ngoc Duyen Tanja Tu]], [[Lukas Weimer]], [[Fotis Jannidis]]
library-catalog:: ids-pub.bsz-bw.de
links:: [Local library](zotero://select/groups/2386895/items/NSE225ZJ), [Web library](https://www.zotero.org/groups/2386895/items/NSE225ZJ)

- [[Abstract]]
	- We present recognizers for four very different types of speech, thought and writing representation (STWR) for German texts. The implementation is based on deep learning with two different customized contextual embeddings, namely FLAIR embeddings and BERT embeddings. This paper gives an evaluation of our recognizers with a particular focus on the differences in performance we observed between those two embeddings. FLAIR performed best for direct STWR (F1=0.85), BERT for indirect (F1=0.76) and free indirect (F1=0.59) STWR. For reported STWR, the comparison was inconclusive, but BERT gave the best average results and best individual model (F1=0.60). Our best recognizers, our customized language embeddings and most of our test and training data are freely available and can be found via www.redewiedergabe.de or at github.com/redewiedergabe.
- [[Attachments]]
	- [Full Text PDF](https://ids-pub.bsz-bw.de/files/11561/Brunner_Tu_Comparing_contextual_embeddings_2020.pdf) {{zotero-imported-file XKWG7R7J, "Brunner et al. - 2020 - To BERT or not to BERT – Comparing contextual embe.pdf"}}