tags:: [[AnalyzeStatistically]], [[bigdata]], [[meta_Theorizing]], [[t_MachineLearning]], [[t_Stylometry]]
date:: 2010/06
issue:: 2
doi:: 10.1093/llc/fqq001
title:: @A comparative study of machine learning methods for authorship attribution
pages:: 215-223
volume:: 25
item-type:: [[journalArticle]]
access-date:: 2011-12-14T09:35:39Z
original-title:: A comparative study of machine learning methods for authorship attribution
language:: en
url:: http://llc.oxfordjournals.org/content/25/2/215.abstract
publication-title:: Literary and Linguistic Computing
authors:: [[Matthew L. Jockers]], [[Daniela M. Witten]]
links:: [Local library](zotero://select/groups/2386895/items/NBND94TJ), [Web library](https://www.zotero.org/groups/2386895/items/NBND94TJ)

- [[Abstract]]
	- We compare and benchmark the performance of five classification methods, four of which are taken from the machine learning literature, in a classic authorship attribution problem involving the Federalist Papers. Cross-validation results are reported for each method, and each method is further employed in classifying the disputed papers and the few papers that are generally understood to be coauthored. These tests are performed using two separate feature sets: a “raw” feature set containing all words and word bigrams that are common to all of the authors, and a second “pre-processed” feature set derived by reducing the raw feature set to include only words meeting a minimum relative frequency threshold. Each of the methods tested performed well, but nearest shrunken centroids and regularized discriminant analysis had the best overall performances with 0/70 cross-validation errors.
- [[Attachments]]
	- [Snapshot](http://llc.oxfordjournals.org/content/25/2/215.abstract) {{zotero-imported-file FGH8KLKB, "215.html"}}