tags:: [[Computer Science - Computation and Language]]
date:: [[Mar 4th, 2016]]
extra:: arXiv: 1603.01360
title:: @Neural Architectures for Named Entity Recognition
item-type:: [[journalArticle]]
access-date:: 2018-10-23T20:07:35Z
original-title:: Neural Architectures for Named Entity Recognition
url:: http://arxiv.org/abs/1603.01360
publication-title:: arXiv:1603.01360 [cs]
authors:: [[Guillaume Lample]], [[Miguel Ballesteros]], [[Sandeep Subramanian]], [[Kazuya Kawakami]], [[Chris Dyer]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/groups/2386895/items/NN7H84GN), [Web library](https://www.zotero.org/groups/2386895/items/NN7H84GN)

- [[Abstract]]
	- State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.
- [[Attachments]]
	- [arXiv.org Snapshot](https://arxiv.org/abs/1603.01360) {{zotero-imported-file ASPK5445, "1603.html"}}
	- [arXiv:1603.01360 PDF](http://www.arxiv.org/pdf/1603.01360.pdf) {{zotero-imported-file 5X46D748, "Lample et al. - 2016 - Neural Architectures for Named Entity Recognition.pdf"}}
- [[Notes]]
	- Comment: Proceedings of NAACL 2016