date:: [[Jun 25th, 2006]]
publisher:: Association for Computing Machinery
place:: "New York, NY, USA"
series:: ICML '06
proceedings-title:: Proceedings of the 23rd international conference on Machine learning
isbn:: 978-1-59593-383-6
doi:: 10.1145/1143844.1143891
title:: @Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks
pages:: 369–376
item-type:: [[conferencePaper]]
access-date:: 2021-08-05
original-title:: Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks
url:: https://doi.org/10.1145/1143844.1143891
short-title:: Connectionist temporal classification
authors:: [[Alex Graves]], [[Santiago Fernández]], [[Faustino Gomez]], [[Jürgen Schmidhuber]]
library-catalog:: ACM Digital Library
links:: [Local library](zotero://select/groups/2386895/items/IN3HKYQZ), [Web library](https://www.zotero.org/groups/2386895/items/IN3HKYQZ)

- [[Abstract]]
	- Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.