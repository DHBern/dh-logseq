tags:: [[Computer Science - Computation and Language]], [[Computer Science - Machine Learning]]
date:: [[Apr 9th, 2021]]
extra:: arXiv: 2010.12309
title:: @A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios
item-type:: [[journalArticle]]
access-date:: 2021-08-12T12:05:11Z
original-title:: A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios
url:: http://arxiv.org/abs/2010.12309
publication-title:: arXiv:2010.12309 [cs]
authors:: [[Michael A. Hedderich]], [[Lukas Lange]], [[Heike Adel]], [[Jannik Str√∂tgen]], [[Dietrich Klakow]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/groups/2386895/items/PDGX7B6A), [Web library](https://www.zotero.org/groups/2386895/items/PDGX7B6A)

- [[Abstract]]
	- Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.
- [[Notes]]
	- Comment: Accepted at NAACL 2021