date:: 2019-08
publisher:: Association for Computational Linguistics
place:: "Florence, Italy"
proceedings-title:: Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)
doi:: 10.18653/v1/W19-4312
title:: @Towards Robust Named Entity Recognition for Historic German
pages:: 96â€“103
item-type:: [[conferencePaper]]
access-date:: 2022-01-21T07:19:14Z
original-title:: Towards Robust Named Entity Recognition for Historic German
url:: https://aclanthology.org/W19-4312
authors:: [[Stefan Schweter]], [[Johannes Baiter]]
library-catalog:: ACLWeb
links:: [Local library](zotero://select/groups/2386895/items/5AB5LTCC), [Web library](https://www.zotero.org/groups/2386895/items/5AB5LTCC)

- [[Abstract]]
	- In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6%.
- [[Attachments]]
	- [Full Text PDF](https://aclanthology.org/W19-4312.pdf) {{zotero-imported-file AKA4MF8W, "Schweter und Baiter - 2019 - Towards Robust Named Entity Recognition for Histor.pdf"}}