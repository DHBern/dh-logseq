tags:: [[Computer Science - Computation and Language]]
date:: [[Aug 28th, 2016]]
extra:: arXiv: 1608.07836
title:: @What to do about non-standard (or non-canonical) language in NLP
item-type:: [[journalArticle]]
access-date:: 2022-01-12T09:28:44Z
original-title:: What to do about non-standard (or non-canonical) language in NLP
url:: http://arxiv.org/abs/1608.07836
publication-title:: arXiv:1608.07836 [cs]
authors:: [[Barbara Plank]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/groups/2386895/items/7LNGR4N5), [Web library](https://www.zotero.org/groups/2386895/items/7LNGR4N5)

- [[Abstract]]
	- Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community's approach to language. I argue for leveraging what I call fortuitous data, i.e., non-obvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.
- [[Notes]]
	- Comment: KONVENS 2016